[
  {
    "title": "Turbocharging Vector Databases using Modern SSDs",
    "venue": "VLDB",
    "venue_full": "Proceedings of the 51st International Conference on Very Large Data Bases",
    "date": "2025-08-01",
    "authors": ["Joobo Shim*", "Jaewon Oh", "Hongchan Roh", "Jaeyoung Do^", "Sang-Won Lee"],
    "abstract": "Efficient and scalable vector search is critical for modern AI applications, particularly in retrieval-augmented generation (RAG) and large-scale semantic search. However, disk-based vector databases often suffer from significant I/O bottlenecks due to suboptimal cache hit ratios and inefficient use of modern SSD architectures. In this work, we introduce a suite of optimizations to enhance the performance of disk-resident Approximate Nearest Neighbor (ANN) indices, specifically focusing on hierarchical graph-based indexing such as HNSW. Our approach leverages three key strategies: (1) Parallel I/O leveraging io_uring to exploit SSD concurrency and reduce retrieval latency, (2) Spatially-aware insertion reordering to improve cache efficiency by dynamically adjusting insert execution order based on locality, and (3) Locality-preserving colocation to restructure index layouts and minimize costly random disk accesses. We implement these techniques within pgvector, a PostgreSQL extension for vector search, and conduct extensive evaluations using real-world datasets. Our optimizations yield up to 11.1× improvement in query throughput, a 3.23× increase in cache hit ratio, and a 98.4% reduction in index build time. Moreover, our findings underscore the importance of SSD-aware indexing strategies for scalable vector retrieval. By integrating hardware-aware I/O optimizations with intelligent data placement techniques, this work paves the way for more efficient, high-performance disk-based vector search engines that could fully leverage modern SSD’s high parallelism."
  },
  {
    "title": "SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding",
    "venue": "ICML",
    "venue_full": "The International Conference on Machine Learning",
    "date": "2025-07-01",
    "authors": ["Woohyeon Park*", "Woojin Kim", "Jaeik Kim", "Jaeyoung Do^"],
    "abstract": "Despite significant advancements in Vision-Language Models (VLMs), the performance of existing VLMs remains hindered by object hallucination, a critical challenge to achieving accurate visual understanding. To address this issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach that enables VLMs to effectively leverage multi-scale visual information with an object-centric manner, closely aligning with human visual perception. SECOND progressively selects and integrates multi-scale visual information, facilitating a more precise interpretation of images. By contrasting these visual information iteratively, SECOND significantly reduces perceptual hallucinations and outperforms a wide range of benchmarks. Our theoretical analysis and experiments highlight the largely unexplored potential of multi-scale application in VLMs, showing that prioritizing and contrasting across scales outperforms existing methods.",
    "paper": "https://arxiv.org/abs/2506.08391"
  },
  {
    "title": "MathReader: Text-to-Speech for Mathematical Documents",
    "venue": "ICASSP",
    "venue_full": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "date": "2025-04-01",
    "authors": ["Sieun Hyeon*", "Kyudan Jung", "Nam-Joon Kim", "Hyun Gon Ryu", "Jaeyoung Do^"],
    "abstract": "TTS (Text-to-Speech) document reader from Microsoft, Adobe, Apple, and OpenAI have been serviced worldwide. They provide relatively good TTS results for general plain text, but sometimes skip contents or provide unsatisfactory results for mathematical expressions. This is because most modern academic papers are written in LaTeX, and when LaTeX formulas are compiled, they are rendered as distinctive text forms within the document. However, traditional TTS document readers output only the text as it is recognized, without considering the mathematical meaning of the formulas. To address this issue, we propose MathReader, which effectively integrates OCR, a fine-tuned T5 model, and TTS. MathReader demonstrated a lower Word Error Rate (WER) than existing TTS document readers, such as Microsoft Edge and Adobe Acrobat, when processing documents containing mathematical formulas. MathReader reduced the WER from 0.510 to 0.281 compared to Microsoft Edge, and from 0.617 to 0.281 compared to Adobe Acrobat. This will significantly contribute to alleviating the inconvenience faced by users who want to listen to documents, especially those who are visually impaired.",
    "code": "https://github.com/hyeonsieun/MathReader",
    "paper": "https://arxiv.org/abs/2501.07088",
    "demo": "https://hyeonsieun.github.io/MathReader_demo/"
  },
  {
    "title": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical Speech-to-Formula",
    "venue": "AAAI",
    "venue_full": "Proceedings of the 39th Annual AAAI Conference on Artificial Intelligence",
    "date": "2025-02-01",
    "authors": ["Sieun Hyeon*", "Kyudan Jung*", "Jaehee Won", "Nam-Joon Kim", "Hyun Gon Ryu", "Hyuk-Jae Lee", "Jaeyoung Do^"],
    "abstract": "In various academic and professional settings, such as mathematics lectures or research presentations, it is often necessary to convey mathematical expressions orally. However, reading mathematical expressions aloud without accompanying visuals can significantly hinder comprehension, especially for those who are hearing-impaired or rely on subtitles due to language barriers. For instance, when a presenter reads Euler's Formula, current Automatic Speech Recognition (ASR) models often produce a verbose and error-prone textual description (e.g., e to the power of i x equals cosine of x plus i side of x), instead of the concise LATEX format (i.e., eix=cos(x)+isin(x)), which hampers clear understanding and communication. To address this issue, we introduce MathSpeech, a novel pipeline that integrates ASR models with small Language Models (sLMs) to correct errors in mathematical expressions and accurately convert spoken expressions into structured LATEX representations. Evaluated on a new dataset derived from lecture recordings, MathSpeech demonstrates LATEX generation capabilities comparable to leading commercial Large Language Models (LLMs), while leveraging fine-tuned small language models of only 120M parameters. Specifically, in terms of CER, BLEU, and ROUGE scores for LATEX translation, MathSpeech demonstrated significantly superior capabilities compared to GPT-4o. We observed a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores compared to GPT-4o.",
    "code": "https://github.com/hyeonsieun/MathSpeech",
    "paper": "https://arxiv.org/abs/2412.15655"
  },
  {
    "title": "Aligning Large Language Models via Fine-grained Supervision",
    "venue": "ACL",
    "venue_full": "Proceedings of the Annual Meeting Of The Association For Computational Linguistics",
    "date": "2024-08-01",
    "authors": ["Dehong Xu", "Liang Qiu", "Minseok Kim", "Faisal Ladhak", "Jaeyoung Do"],
    "abstract": "Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision. Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can achieve up to an absolute improvement of 5.1% in LLM performance, in terms of win rate against the reference model, compared with the traditional PPO model.",
    "paper": "https://arxiv.org/abs/2406.02756"
  },
  {
    "title": "AscleAI: A LLM-based clinical note management system for enhancing clinician productivity",
    "venue": "CHI",
    "venue_full": "Proceedings of the CHI Conference on Human Factors in Computing Systems",
    "date": "2024-05-01",
    "authors": ["Jiyeon Han", "Jimin Park", "Jinyoung Huh", "Uran Oh", "Daehee Kim", "Jaeyoung Do"]
  },
  {
    "title": "Weakly supervised referring image segmentation with intra-chunk and inter-chunk consistency",
    "venue": "ICCV",
    "venue_full": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    "date": "2023-10-01",
    "authors": ["Jungbeom Lee", "Sungjin Lee", "Jinseok Nam", "Seunghak Yu", "Jaeyoung Do", "Tara Taghavi"]
  },
  {
    "title": "Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems",
    "venue": "ACL",
    "venue_full": "Proceedings of the 61st Annual Meeting Of The Association For Computational Linguistics",
    "date": "2023-07-01",
    "authors": ["Sarthak Ahuja", "Mohammad Kachuee", "Fatemeh Sheikholeslami", "Weiqing Liu", "Jaeyoung Do"]
  },
  {
    "title": "Large-scale lifelong learning of in-context instructions and how to tackle it",
    "venue": "ACL",
    "venue_full": "Proceedings of the 61st Annual Meeting Of The Association For Computational Linguistics",
    "date": "2023-07-01",
    "authors": ["Jisoo Mok", "Jaeyoung Do", "Sungjin Lee", "Tara Taghavi", "Seunghak Yu", "Sungroh Yoon"]
  },
  {
    "title": "Grounding counterfactual explanation of image classifiers to textual concept space",
    "venue": "CVPR",
    "venue_full": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    "date": "2023-06-01",
    "authors": ["Siwon Kim", "Jinoh Oh", "Sungjin Lee", "Seunghak Yu", "Jaeyoung Do", "Tara Taghavi"]
  },
  {
    "title": "Extending and programming the NVMe I/O determinism interface for Flash Arrays",
    "venue": "ToS",
    "venue_full": "ACM Transactions on Storage",
    "date": "2023-01-01",
    "authors": ["Huaicheng Li", "Martin L Putra", "Ronald Shi", "Fadhil I Kurnia", "Xing Lin", "Jaeyoung Do", "Achmad Imam Kistijantoro", "Gregory R Ganger", "Haryadi S Gunawi"]
  },
  {
    "title": "Accelerating Large-Scale Graph-Based Nearest Neighbor Search on a Computational Storage Platform",
    "venue": "ToC",
    "venue_full": "IEEE Transactions on Computers",
    "date": "2023-01-01",
    "authors": ["Ji-Hoon Kim", "Yeo-Reum Park", "Jaeyoung Do", "Soo-Young Ji", "Joo-Young Kim"]
  },
  {
    "title": "Debiasing neighbor aggregation for graph neural network in recommender systems",
    "venue": "CIKM",
    "venue_full": "Proceedings of the ACM International Conference on Information & Knowledge Management",
    "date": "2022-10-01",
    "authors": ["Minseok Kim", "Jinoh Oh", "Jaeyoung Do", "Sungjin Lee"]
  },
  {
    "title": "A Dual-Mode Similarity Search Accelerator based on Embedding Compression for Online Cross-Modal Image-Text Retrieval",
    "venue": "FCCM",
    "venue_full": "IEEE 30th Annual International Symposium on Field-Programmable Custom Computing Machines",
    "date": "2022-05-01",
    "authors": ["Yeo-Reum Park", "Ji-Hoon Kim", "Jaeyoung Do", "Joo-Young Kim"]
  },
  {
    "title": "Accelerating large-scale nearest neighbor search with computational storage device",
    "venue": "FCCM",
    "venue_full": "IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines",
    "date": "2021-05-01",
    "authors": ["Ji-Hoon Kim", "Yeo-Reum Park", "Jaeyoung Do", "Soo-Young Ji", "Joo-Young Kim"]
  },
  {
    "title": "Programming an SSD controller to support batched writes for variable-size pages",
    "venue": "ICDE",
    "venue_full": "IEEE 37th International Conference on Data Engineering",
    "date": "2021-04-01",
    "authors": ["Jaeyoung Do", "Chen Luo", "David Lomet"]
  },
  {
    "title": "Better database cost/performance via batched I/O on programmable SSD",
    "venue": "VLDB J.",
    "venue_full": "The VLDB Journal",
    "date": "2021-02-01",
    "authors": ["Jaeyoung Do", "Ivan Luiz Picoli", "David Lomet", "Philippe Bonnet"]
  },
  {
    "title": "Computational storage: Where are we today?",
    "venue": "CIDR",
    "venue_full": "11th Conference on Innovative Data Systems Research",
    "date": "2021-01-01",
    "authors": ["Antonio Barbalace", "Jaeyoung Do"]
  },
  {
    "title": "Cost-effective, energy-efficient, and scalable storage computing for large-scale AI applications",
    "venue": "TOS",
    "venue_full": "ACM Transactions on Storage",
    "date": "2020-10-01",
    "authors": ["Jaeyoung Do", "Victor C Ferreira", "Hossein Bobarshad", "Mahdi Torabzadehkashi", "Siavash Rezaei", "Ali Heydarigorji", "Diego Souza", "Brunno F Goldstein", "Leandro Santiago", "Min Soo Kim", "Priscila MV Lima", "Felipe MG França", "Vladimir Alves"]
  },
  {
    "title": "Lessons learned from the early performance evaluation of intel optane dc persistent memory in dbms",
    "venue": "DaMoN",
    "venue_full": "Proceedings of the 16th International Workshop on Data Management on New Hardware",
    "date": "2020-01-01",
    "authors": ["Yinjun Wu", "Kwanghyun Park", "Rathijit Sen", "Brian Kroth", "Jaeyoung Do"]
  },
  {
    "title": "ALEX: an updatable adaptive learned index",
    "venue": "SIGMOD",
    "venue_full": "Proceedings of the ACM SIGMOD International Conference on Management of Data",
    "date": "2020-05-01",
    "authors": ["Jialin Ding", "Umar Farooq Minhas", "Jia Yu", "Chi Wang", "Jaeyoung Do", "Yinan Li", "Hantian Zhang", "Badrish Chandramouli", "Johannes Gehrke", "Donald Kossmann", "David Lomet", "Tim Kraska"]
  },
  {
    "title": "Improving CPU I/O performance via SSD controller FTL support for batched writes",
    "venue": "DaMoN",
    "venue_full": "Proceedings of the 15th International Workshop on Data Management on New Hardware",
    "date": "2019-07-01",
    "authors": ["Jaeyoung Do", "David Lomet", "Ivan Luiz Picoli"]
  },
  {
    "title": "Programmable solid-state storage in future cloud datacenters",
    "venue": "CACM",
    "venue_full": "Communications of the ACM",
    "date": "2019-05-01",
    "authors": ["Jaeyoung Do", "Sudipta Sengupta", "Steven Swanson"]
  },
  {
    "title": "Aggressive buffer pool warm-up after restart in SQL Server",
    "venue": "ICDEW",
    "venue_full": "IEEE International Conference on Data Engineering Workshops",
    "date": "2016-05-01",
    "authors": ["Kwanghyun Park", "Jaeyoung Do", "Nikhil Teletia", "Jignesh M Patel"]
  },
  {
    "title": "Query Processing on Smart SSDs.",
    "venue": "Data Eng. Bull.",
    "venue_full": "IEEE Data Engeering Bulletin",
    "date": "2014-06-01",
    "authors": ["Kwanghyun Park", "Yang-Suk Kee", "Jignesh M Patel", "Jaeyoung Do", "Chanik Park", "David J Dewitt"]
  },
  {
    "title": "Query processing on smart ssds: Opportunities and challenges",
    "venue": "SIGMOD",
    "venue_full": "Proceedings of the ACM International Conference on Management of Data",
    "date": "2013-06-01",
    "authors": ["Jaeyoung Do", "Yang-Suk Kee", "Jignesh M Patel", "Chanik Park", "Kwanghyun Park", "David J DeWitt"]
  },
  {
    "title": "Fast peak-to-peak behavior with SSD buffer pool",
    "venue": "ICDE",
    "venue_full": "IEEE International Conference on Data Engineering",
    "date": "2013-04-01",
    "authors": ["Jaeyoung Do", "Donghui Zhang", "Jignesh M Patel", "David J DeWitt"]
  },
  {
    "title": "Turbocharging DBMS buffer pool using SSDs",
    "venue": "SIGMOD",
    "venue_full": "Proceedings of the ACM SIGMOD International Conference on Management of Data",
    "date": "2011-06-01",
    "authors": ["Jaeyoung Do", "Donghui Zhang", "Jignesh M Patel", "David J DeWitt", "Jeffrey F Naughton", "Alan Halverson"]
  },
  {
    "title": "Join processing for flash SSDs: remembering past lessons",
    "venue": "DaMoN",
    "venue_full": "Proceedings of the Fifth International Workshop on Data Management on New Hardware",
    "date": "2009-06-01",
    "authors": ["Jaeyoung Do", "Jignesh M Patel"]
  },
  {
    "title": "Fast statistical alignment",
    "venue": "PLoS Comput. Biol.",
    "venue_full": "PLoS Computational Biology",
    "date": "2009-05-01",
    "authors": ["Robert K Bradley", "Adam Roberts", "Michael Smoot", "Sudeep Juvekar", "Jaeyoung Do", "Colin Dewey", "Ian Holmes", "Lior Pachter"]
  }
]

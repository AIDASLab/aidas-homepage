[
    {
    "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models",
    "venue": "ICLR",
    "venue_full": "The Fourteenth International Conference on Learning Representations",
    "date": "2026-04-23",
    "authors": ["Yunseok Han*", "Yejoon Lee", "Jaeyoung Do^"],
    "abstract": "Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from post-intervention stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy–faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process."
  },
  {
    "title": "MatKV: Trading Compute for Flash Storage in LLM Inference",
    "venue": "ICDE",
    "venue_full": "42nd IEEE International Conference on Data Engineering",
    "date": "2026-05-04",
    "authors": ["Kun-Woo Shin*", "Jay H. Park", "Moonwook Oh", "Yohan Jo", "Jaeyoung Do", "Sang-Won Lee"],
    "abstract": "We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face’s Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without negatively impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV’s potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments."
  },
  {
    "title": "Don’t Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation",
    "venue": "NeurIPS",
    "venue_full": "39th Annual Conference on Neural Information Processing Systems",
    "date": "2025-09-19",
    "authors": ["Woojin Kim*", "Jaeyoung Do^"],
    "abstract": "Classifier guidance is a widely adopted technique in diffusion language models, used to steer generation toward desired attributes. However, such guidance often introduces instability during the generation process, where token-level updates fluctuate across timesteps. We identify and formally characterize this phenomenon as update-forgetting. This instability disrupts the refinement process by overwriting semantic edits, ultimately degrading fluency and coherence, which is particularly problematic in tasks like controllable text generation. To address this, we propose TTA-Diffusion, a novel inference-time approach that dynamically allocates timesteps per token based on refinement needs. Unlike conventional diffusion models that apply uniform updates, TTA-Diffusion employs structured timestep allocation, preserving stable tokens while allowing uncertain tokens to undergo progressive adjustment. Experimental results across diverse tasks demonstrate that TTA-Diffusion significantly outperforms both diffusion-based and auto-regressive baselines in fluency and control accuracy while improving computational efficiency by reducing the number of required timesteps. On the sentiment control task, TTA-Diffusion achieves over 20% higher accuracy and nearly half the perplexity of prior diffusion models, using less than one-fifth the denoising steps. This work highlights the importance of mitigating fluctuations in token updates and promoting a balanced refinement process, thereby enhancing stability and controllability in controllable language modeling.",
    "page": "https://aidaslab.github.io/TTA-Diffusion/",
    "paper": "https://neurips.cc/virtual/2025/loc/san-diego/poster/118561",
    "arXiv": "https://arxiv.org/abs/2510.26200",
    "code": "https://github.com/AIDASLab/TTA-Diffusion"
  },
  {
    "title": "Exploring and Leveraging Class Vectors for Classifier Editing",
    "venue": "NeurIPS",
    "venue_full": "39th Annual Conference on Neural Information Processing Systems",
    "date": "2025-09-19",
    "authors": ["Jaeik Kim*", "Jaeyoung Do^"],
    "abstract": "Image classifiers play a critical role in detecting diseases in medical imaging and identifying anomalies in manufacturing processes. However, their predefined behaviors after extensive training make post hoc model editing difficult, especially when it comes to forgetting specific classes or adapting to distribution shifts. Existing classifier editing methods either focus narrowly on correcting errors or incur extensive retraining costs, creating a bottleneck for flexible editing. Moreover, such editing has seen limited investigation in image classification. To overcome these challenges, we introduce class vectors, which capture class-specific representation adjustments during fine-tuning. Whereas task vectors encode task-level changes in weight space, class vectors disentangle each class’s adaptation in the latent space. We show that class vectors capture each class’s semantic shift and that classifier editing can be achieved either by steering latent features along these vectors or by mapping them into weight space to update the decision boundaries. We also demonstrate that the inherent linearity and orthogonality of class vectors support efficient, flexible, and high-level concept editing via simple class arithmetic. Finally, we validate their utility in applications such as unlearning, environmental adaptation, adversarial defense, and adversarial trigger optimization.",
    "page": "https://aidaslab.github.io/Class-Vector/",
    "paper": "https://neurips.cc/virtual/2025/loc/san-diego/poster/116452",
    "arXiv": "https://arxiv.org/abs/2510.11268",
    "code": "https://github.com/AIDASLab/Class-Vector"
  },
  {
    "title": "MMPB: It’s Time for Multi-Modal Personalization",
    "venue": "NeurIPS",
    "venue_full": "39th Annual Conference on Neural Information Processing Systems",
    "date": "2025-09-19",
    "authors": ["Jaeik Kim*", "Woojin Kim", "Woohyeon Park", "Jaeyoung Do^"],
    "abstract": "Visual personalization is essential in user-facing AI systems such as smart homes and healthcare, where aligning model behavior with user-centric concepts is critical. However, recent large Vision Language Models (VLMs), despite their broad applicability, remain underexplored in their ability to adapt to individual users. In this paper, we introduce MMPB, the first extensive benchmark for evaluating VLMs on personalization. MMPB comprises 10k image-query pairs and includes 111 personalizable concepts across four categories: humans, animals, objects, and characters, with the human category enriched with preference-grounded queries. We structure personalization into three main task types, each highlighting a different key property of VLMs. Using 23 widely used VLMs including both open- and closed-source models, we evaluate personalization performance via a three-stage protocol: concept injection, multi-turn dialogue, and personalized querying. Our findings indicate that most VLMs (including some closed-source models) struggle with personalization, particularly in maintaining consistency over dialogue, handling user preferences, and adapting to visual cues. Our analysis reveals that the challenges in VLM personalization (such as refusal behaviors and long-context forgetting) highlight substantial room for improvement. By identifying these limitations and offering a scalable benchmark, MMPB offers valuable insights and a solid foundation for future research toward truly personalized multi-modal AI.",
    "page": "https://aidaslab.github.io/MMPB/",
    "paper": "https://neurips.cc/virtual/2025/loc/san-diego/poster/121766",
    "arXiv": "https://arxiv.org/abs/2509.22820",
    "code": "https://github.com/AIDASLab/MMPB",
    "dataset": "https://huggingface.co/datasets/snu-aidas/MMPB"
  },
  {
    "title": "Turbocharging Vector Databases using Modern SSDs",
    "venue": "VLDB",
    "venue_full": "Proceedings of the 51st International Conference on Very Large Data Bases",
    "date": "2025-08-01",
    "authors": ["Joobo Shim*", "Jaewon Oh", "Hongchan Roh", "Jaeyoung Do^", "Sang-Won Lee"],
    "abstract": "Efficient and scalable vector search is critical for modern AI applications, particularly in retrieval-augmented generation (RAG) and large-scale semantic search. However, disk-based vector databases often suffer from significant I/O bottlenecks due to suboptimal cache hit ratios and inefficient use of modern SSD architectures. In this work, we introduce a suite of optimizations to enhance the performance of disk-resident Approximate Nearest Neighbor (ANN) indices, specifically focusing on hierarchical graph-based indexing such as HNSW. Our approach leverages three key strategies: (1) Parallel I/O leveraging io_uring to exploit SSD concurrency and reduce retrieval latency, (2) Spatially-aware insertion reordering to improve cache efficiency by dynamically adjusting insert execution order based on locality, and (3) Locality-preserving colocation to restructure index layouts and minimize costly random disk accesses. We implement these techniques within pgvector, a PostgreSQL extension for vector search, and conduct extensive evaluations using real-world datasets. Our optimizations yield up to 11.1× improvement in query throughput, a 3.23× increase in cache hit ratio, and a 98.4% reduction in index build time. Moreover, our findings underscore the importance of SSD-aware indexing strategies for scalable vector retrieval. By integrating hardware-aware I/O optimizations with intelligent data placement techniques, this work paves the way for more efficient, high-performance disk-based vector search engines that could fully leverage modern SSD’s high parallelism."
  },
  {
    "title": "SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding",
    "venue": "ICML",
    "venue_full": "The International Conference on Machine Learning",
    "date": "2025-07-01",
    "authors": ["Woohyeon Park*", "Woojin Kim", "Jaeik Kim", "Jaeyoung Do^"],
    "abstract": "Despite significant advancements in Vision-Language Models (VLMs), the performance of existing VLMs remains hindered by object hallucination, a critical challenge to achieving accurate visual understanding. To address this issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach that enables VLMs to effectively leverage multi-scale visual information with an object-centric manner, closely aligning with human visual perception. SECOND progressively selects and integrates multi-scale visual information, facilitating a more precise interpretation of images. By contrasting these visual information iteratively, SECOND significantly reduces perceptual hallucinations and outperforms a wide range of benchmarks. Our theoretical analysis and experiments highlight the largely unexplored potential of multi-scale application in VLMs, showing that prioritizing and contrasting across scales outperforms existing methods.",
    "page": "https://aidaslab.github.io/SECOND/",
    "paper": "https://icml.cc/virtual/2025/poster/45215",
    "arXiv": "https://arxiv.org/abs/2506.08391",
    "code": "https://github.com/AIDASLab/SECOND"
  },
  {
    "title": "MathReader: Text-to-Speech for Mathematical Documents",
    "venue": "ICASSP",
    "venue_full": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "date": "2025-04-01",
    "authors": ["Sieun Hyeon*", "Kyudan Jung", "Nam-Joon Kim", "Hyun Gon Ryu", "Jaeyoung Do^"],
    "abstract": "TTS (Text-to-Speech) document reader from Microsoft, Adobe, Apple, and OpenAI have been serviced worldwide. They provide relatively good TTS results for general plain text, but sometimes skip contents or provide unsatisfactory results for mathematical expressions. This is because most modern academic papers are written in LaTeX, and when LaTeX formulas are compiled, they are rendered as distinctive text forms within the document. However, traditional TTS document readers output only the text as it is recognized, without considering the mathematical meaning of the formulas. To address this issue, we propose MathReader, which effectively integrates OCR, a fine-tuned T5 model, and TTS. MathReader demonstrated a lower Word Error Rate (WER) than existing TTS document readers, such as Microsoft Edge and Adobe Acrobat, when processing documents containing mathematical formulas. MathReader reduced the WER from 0.510 to 0.281 compared to Microsoft Edge, and from 0.617 to 0.281 compared to Adobe Acrobat. This will significantly contribute to alleviating the inconvenience faced by users who want to listen to documents, especially those who are visually impaired.",
    "page": "https://aidaslab.github.io/MathReader/",
    "arXiv": "https://arxiv.org/abs/2501.07088",
    "code": "https://github.com/hyeonsieun/MathReader",
    "demo": "https://hyeonsieun.github.io/MathReader_demo/"
  },
  {
    "title": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical Speech-to-Formula",
    "venue": "AAAI",
    "venue_full": "Proceedings of the 39th Annual AAAI Conference on Artificial Intelligence",
    "date": "2025-02-01",
    "authors": ["Sieun Hyeon*", "Kyudan Jung*", "Jaehee Won", "Nam-Joon Kim", "Hyun Gon Ryu", "Hyuk-Jae Lee", "Jaeyoung Do^"],
    "abstract": "In various academic and professional settings, such as mathematics lectures or research presentations, it is often necessary to convey mathematical expressions orally. However, reading mathematical expressions aloud without accompanying visuals can significantly hinder comprehension, especially for those who are hearing-impaired or rely on subtitles due to language barriers. For instance, when a presenter reads Euler's Formula, current Automatic Speech Recognition (ASR) models often produce a verbose and error-prone textual description (e.g., e to the power of i x equals cosine of x plus i side of x), instead of the concise LATEX format (i.e., eix=cos(x)+isin(x)), which hampers clear understanding and communication. To address this issue, we introduce MathSpeech, a novel pipeline that integrates ASR models with small Language Models (sLMs) to correct errors in mathematical expressions and accurately convert spoken expressions into structured LATEX representations. Evaluated on a new dataset derived from lecture recordings, MathSpeech demonstrates LATEX generation capabilities comparable to leading commercial Large Language Models (LLMs), while leveraging fine-tuned small language models of only 120M parameters. Specifically, in terms of CER, BLEU, and ROUGE scores for LATEX translation, MathSpeech demonstrated significantly superior capabilities compared to GPT-4o. We observed a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores compared to GPT-4o.",
    "page": "https://aidaslab.github.io/MathSpeech/",
    "arXiv": "https://arxiv.org/abs/2412.15655",
    "code": "https://github.com/hyeonsieun/MathSpeech"
  },
  {
    "title": "Aligning Large Language Models via Fine-grained Supervision",
    "venue": "ACL",
    "venue_full": "Proceedings of the Annual Meeting Of The Association For Computational Linguistics",
    "date": "2024-08-01",
    "authors": ["Dehong Xu", "Liang Qiu", "Minseok Kim", "Faisal Ladhak", "Jaeyoung Do"],
    "abstract": "Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision. Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can achieve up to an absolute improvement of 5.1% in LLM performance, in terms of win rate against the reference model, compared with the traditional PPO model.",
    "arXiv": "https://arxiv.org/abs/2406.02756"
  },
  {
    "title": "AscleAI: A LLM-based clinical note management system for enhancing clinician productivity",
    "venue": "CHI",
    "venue_full": "Proceedings of the CHI Conference on Human Factors in Computing Systems",
    "date": "2024-05-01",
    "authors": ["Jiyeon Han", "Jimin Park", "Jinyoung Huh", "Uran Oh", "Daehee Kim", "Jaeyoung Do"]
  },
  {
    "title": "Weakly supervised referring image segmentation with intra-chunk and inter-chunk consistency",
    "venue": "ICCV",
    "venue_full": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    "date": "2023-10-01",
    "authors": ["Jungbeom Lee", "Sungjin Lee", "Jinseok Nam", "Seunghak Yu", "Jaeyoung Do", "Tara Taghavi"]
  },
  {
    "title": "Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems",
    "venue": "ACL",
    "venue_full": "Proceedings of the 61st Annual Meeting Of The Association For Computational Linguistics",
    "date": "2023-07-01",
    "authors": ["Sarthak Ahuja", "Mohammad Kachuee", "Fatemeh Sheikholeslami", "Weiqing Liu", "Jaeyoung Do"]
  },
  {
    "title": "Large-scale lifelong learning of in-context instructions and how to tackle it",
    "venue": "ACL",
    "venue_full": "Proceedings of the 61st Annual Meeting Of The Association For Computational Linguistics",
    "date": "2023-07-01",
    "authors": ["Jisoo Mok", "Jaeyoung Do", "Sungjin Lee", "Tara Taghavi", "Seunghak Yu", "Sungroh Yoon"]
  },
  {
    "title": "Grounding counterfactual explanation of image classifiers to textual concept space",
    "venue": "CVPR",
    "venue_full": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    "date": "2023-06-01",
    "authors": ["Siwon Kim", "Jinoh Oh", "Sungjin Lee", "Seunghak Yu", "Jaeyoung Do", "Tara Taghavi"]
  },
  {
    "title": "Extending and programming the NVMe I/O determinism interface for Flash Arrays",
    "venue": "ToS",
    "venue_full": "ACM Transactions on Storage",
    "date": "2023-01-01",
    "authors": ["Huaicheng Li", "Martin L Putra", "Ronald Shi", "Fadhil I Kurnia", "Xing Lin", "Jaeyoung Do", "Achmad Imam Kistijantoro", "Gregory R Ganger", "Haryadi S Gunawi"]
  },
  {
    "title": "Accelerating Large-Scale Graph-Based Nearest Neighbor Search on a Computational Storage Platform",
    "venue": "ToC",
    "venue_full": "IEEE Transactions on Computers",
    "date": "2023-01-01",
    "authors": ["Ji-Hoon Kim", "Yeo-Reum Park", "Jaeyoung Do", "Soo-Young Ji", "Joo-Young Kim"]
  },
  {
    "title": "Debiasing neighbor aggregation for graph neural network in recommender systems",
    "venue": "CIKM",
    "venue_full": "Proceedings of the ACM International Conference on Information & Knowledge Management",
    "date": "2022-10-01",
    "authors": ["Minseok Kim", "Jinoh Oh", "Jaeyoung Do", "Sungjin Lee"]
  },
  {
    "title": "A Dual-Mode Similarity Search Accelerator based on Embedding Compression for Online Cross-Modal Image-Text Retrieval",
    "venue": "FCCM",
    "venue_full": "IEEE 30th Annual International Symposium on Field-Programmable Custom Computing Machines",
    "date": "2022-05-01",
    "authors": ["Yeo-Reum Park", "Ji-Hoon Kim", "Jaeyoung Do", "Joo-Young Kim"]
  },
  {
    "title": "Accelerating large-scale nearest neighbor search with computational storage device",
    "venue": "FCCM",
    "venue_full": "IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines",
    "date": "2021-05-01",
    "authors": ["Ji-Hoon Kim", "Yeo-Reum Park", "Jaeyoung Do", "Soo-Young Ji", "Joo-Young Kim"]
  },
  {
    "title": "Programming an SSD controller to support batched writes for variable-size pages",
    "venue": "ICDE",
    "venue_full": "IEEE 37th International Conference on Data Engineering",
    "date": "2021-04-01",
    "authors": ["Jaeyoung Do", "Chen Luo", "David Lomet"]
  },
  {
    "title": "Better database cost/performance via batched I/O on programmable SSD",
    "venue": "VLDB J.",
    "venue_full": "The VLDB Journal",
    "date": "2021-02-01",
    "authors": ["Jaeyoung Do", "Ivan Luiz Picoli", "David Lomet", "Philippe Bonnet"]
  },
  {
    "title": "Computational storage: Where are we today?",
    "venue": "CIDR",
    "venue_full": "11th Conference on Innovative Data Systems Research",
    "date": "2021-01-01",
    "authors": ["Antonio Barbalace", "Jaeyoung Do"]
  },
  {
    "title": "Cost-effective, energy-efficient, and scalable storage computing for large-scale AI applications",
    "venue": "TOS",
    "venue_full": "ACM Transactions on Storage",
    "date": "2020-10-01",
    "authors": ["Jaeyoung Do", "Victor C Ferreira", "Hossein Bobarshad", "Mahdi Torabzadehkashi", "Siavash Rezaei", "Ali Heydarigorji", "Diego Souza", "Brunno F Goldstein", "Leandro Santiago", "Min Soo Kim", "Priscila MV Lima", "Felipe MG França", "Vladimir Alves"]
  },
  {
    "title": "Lessons learned from the early performance evaluation of intel optane dc persistent memory in dbms",
    "venue": "DaMoN",
    "venue_full": "Proceedings of the 16th International Workshop on Data Management on New Hardware",
    "date": "2020-01-01",
    "authors": ["Yinjun Wu", "Kwanghyun Park", "Rathijit Sen", "Brian Kroth", "Jaeyoung Do"]
  },
  {
    "title": "ALEX: an updatable adaptive learned index",
    "venue": "SIGMOD",
    "venue_full": "Proceedings of the ACM SIGMOD International Conference on Management of Data",
    "date": "2020-05-01",
    "authors": ["Jialin Ding", "Umar Farooq Minhas", "Jia Yu", "Chi Wang", "Jaeyoung Do", "Yinan Li", "Hantian Zhang", "Badrish Chandramouli", "Johannes Gehrke", "Donald Kossmann", "David Lomet", "Tim Kraska"]
  },
  {
    "title": "Improving CPU I/O performance via SSD controller FTL support for batched writes",
    "venue": "DaMoN",
    "venue_full": "Proceedings of the 15th International Workshop on Data Management on New Hardware",
    "date": "2019-07-01",
    "authors": ["Jaeyoung Do", "David Lomet", "Ivan Luiz Picoli"]
  },
  {
    "title": "Programmable solid-state storage in future cloud datacenters",
    "venue": "CACM",
    "venue_full": "Communications of the ACM",
    "date": "2019-05-01",
    "authors": ["Jaeyoung Do", "Sudipta Sengupta", "Steven Swanson"]
  },
  {
    "title": "Aggressive buffer pool warm-up after restart in SQL Server",
    "venue": "ICDEW",
    "venue_full": "IEEE International Conference on Data Engineering Workshops",
    "date": "2016-05-01",
    "authors": ["Kwanghyun Park", "Jaeyoung Do", "Nikhil Teletia", "Jignesh M Patel"]
  },
  {
    "title": "Query Processing on Smart SSDs.",
    "venue": "Data Eng. Bull.",
    "venue_full": "IEEE Data Engeering Bulletin",
    "date": "2014-06-01",
    "authors": ["Kwanghyun Park", "Yang-Suk Kee", "Jignesh M Patel", "Jaeyoung Do", "Chanik Park", "David J Dewitt"]
  },
  {
    "title": "Query processing on smart ssds: Opportunities and challenges",
    "venue": "SIGMOD",
    "venue_full": "Proceedings of the ACM International Conference on Management of Data",
    "date": "2013-06-01",
    "authors": ["Jaeyoung Do", "Yang-Suk Kee", "Jignesh M Patel", "Chanik Park", "Kwanghyun Park", "David J DeWitt"]
  },
  {
    "title": "Fast peak-to-peak behavior with SSD buffer pool",
    "venue": "ICDE",
    "venue_full": "IEEE International Conference on Data Engineering",
    "date": "2013-04-01",
    "authors": ["Jaeyoung Do", "Donghui Zhang", "Jignesh M Patel", "David J DeWitt"]
  },
  {
    "title": "Turbocharging DBMS buffer pool using SSDs",
    "venue": "SIGMOD",
    "venue_full": "Proceedings of the ACM SIGMOD International Conference on Management of Data",
    "date": "2011-06-01",
    "authors": ["Jaeyoung Do", "Donghui Zhang", "Jignesh M Patel", "David J DeWitt", "Jeffrey F Naughton", "Alan Halverson"]
  },
  {
    "title": "Join processing for flash SSDs: remembering past lessons",
    "venue": "DaMoN",
    "venue_full": "Proceedings of the Fifth International Workshop on Data Management on New Hardware",
    "date": "2009-06-01",
    "authors": ["Jaeyoung Do", "Jignesh M Patel"]
  },
  {
    "title": "Fast statistical alignment",
    "venue": "PLoS Comput. Biol.",
    "venue_full": "PLoS Computational Biology",
    "date": "2009-05-01",
    "authors": ["Robert K Bradley", "Adam Roberts", "Michael Smoot", "Sudeep Juvekar", "Jaeyoung Do", "Colin Dewey", "Ian Holmes", "Lior Pachter"]
  }
]

---
title: "AIDAS Seminar"
date: "2026-02-06"
summary: "AIDAS Seminar"
thumbnail: "seminar/thumbnails/aidas_seminar.png"
---

## Overview

Ongoing weekly reading seminars and technical presentations by AIDAS members.


## Sessions

### 2026 Spring (26-1)
|Date|Category|Topic|Presenter|
|:---:|:---:|:---:|:---:|
|**Feb 6**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#ecfeff;color:#155e75;border:1px solid #a5f3fc;">Agent Systems</span>|Latent Collaboration in Multi-Agent Systems|Sieun Hyeon|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#ecfdf5;color:#166534;border:1px solid #86efac;">Robotics</span>|Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots|Hyeonggeun Kim|
|**Jan 30**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#f5f3ff;color:#5b21b6;border:1px solid #ddd6fe;">Reasoning</span>|Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning|Woohyeon Kim|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#ecfdf5;color:#166534;border:1px solid #86efac;">Robotics</span>|Enhancing intention prediction and interpretability in service robots with LLM and KG|Minwoo Kim|
|**Jan 23**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Progressive Trajectory Matching for Dataset Distillation](https://youtu.be/5Jibd_YvswQ)|Steve Cho|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#f5f3ff;color:#5b21b6;border:1px solid #ddd6fe;">Reasoning</span>|[Reasoning Models Generate Societies of Thought](https://youtu.be/XX73spMVPiw)|Yongha Lee|
|**Jan 16**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[K-Foundation Models](https://youtu.be/matpvMe6ZYY)|Jusang Oh|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://youtu.be/pJKT5WTyZGs)|Jinhyeok Kim|
|**Jan 9**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#faf5ff;color:#6b21a8;border:1px solid #e9d5ff;">Knowledge Graphs</span>|[OntoTune: Ontology-Driven Self-training for Aligning LLMs](https://youtu.be/sEL0uQknz24)|Minteak Lim|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[AI models collapse when trained on recursively generated data](https://youtu.be/3Ua_EujuO2s)|Jaeik Kim|
|**Jan 2**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://youtu.be/6M6TFagLvyE)|Woojin Kim|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[A Path Towards Autonomous Machine Intelligence](https://youtu.be/4mTzt-fQZMg)|Jihwan Hong|

### 2025 Fall (25-2)
|Date|Category|Topic|Presenter|
|:---:|:---:|:---:|:---:|
|**Dec 19**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Numerical mismatch in LLM RL](https://youtu.be/HMQbISeY_n0)|Yunseok Han|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eef2ff;color:#4338ca;border:1px solid #c7d2fe;">Diffusion Models</span>|[TiDAR: Think in Diffusion, Talk in Autoregression](https://youtu.be/0vKHhMbAR6A)|Hanjun Lee|
|**Dec 12**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#f5f3ff;color:#5b21b6;border:1px solid #ddd6fe;">Reasoning</span>|[Reward Reasoning Model](https://youtu.be/Q20uG0Xsn18)|Hoeun Lee|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eef2ff;color:#3730a3;border:1px solid #c7d2fe;">Data Systems</span>|[LightRAG: Simple and Fast Retrieval-Augmented Generation](https://youtu.be/9eJVZ05LcR0)|Geon Choi|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eef2ff;color:#3730a3;border:1px solid #c7d2fe;">Data Systems</span>|[HiFC: High-efficiency Flash-based KV Cache Swapping for Scaling LLM Inference](https://youtu.be/wJnABzHG-Sc)|Dogeun Kim|
|**Dec 5**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#f5f3ff;color:#5b21b6;border:1px solid #ddd6fe;">Reasoning</span>|[Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning](https://youtu.be/liyZDEnlp5M)|Yejoon Lee|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eef2ff;color:#3730a3;border:1px solid #c7d2fe;">Data Systems</span>|[HiFC: High-efficiency Flash-based KV Cache Swapping for Scaling LLM Inference](https://youtu.be/wJnABzHG-Sc)|Dogeun Kim|
|**Nov 28**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs](https://youtu.be/1uO2fM51IFk)|Sieun Hyeon|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#ecfdf5;color:#166534;border:1px solid #86efac;">Robotics</span>|[Robot Learning: A Tutorial](https://youtu.be/Y9RAy03HnCk)|Hyeonggeun Kim|
|**Nov 21**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#fefce8;color:#854d0e;border:1px solid #fde68a;">Reinforcement Learning</span>|[Mastering the game of Go with deep neural networks and tree search](https://youtu.be/IV4_gNvDMdY)|Woohyeon Park|
|**Nov 14**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#f5f3ff;color:#5b21b6;border:1px solid #ddd6fe;">Reasoning</span>|[ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning](https://youtu.be/28qsnscO0Mk)|Yongha Lee|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#faf5ff;color:#6b21a8;border:1px solid #e9d5ff;">Knowledge Graphs</span>|[A Survey on Knowledge Graphs: Representation, Acquisition, and Applications](https://youtu.be/pZTLkm9Eg6Y)|Minwoo Kim|
|**Nov 7**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eef2ff;color:#4338ca;border:1px solid #c7d2fe;">Diffusion Models</span>|[SparseD: Sparse Attention for Diffusion Language Models](https://youtu.be/lgI89eAFjLc)|Jinhyeok Kim|
|**Oct 31**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#f0fdfa;color:#115e59;border:1px solid #99f6e4;">Representation Learning</span>|[Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations](https://youtu.be/EcLrzBV2A3I)|Jusang Oh|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Emergent Introspective Awareness in Large Language Models](https://youtu.be/0DTlY0UFXsY)|Steve Cho|
|**Oct 24**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#ecfdf5;color:#166534;border:1px solid #86efac;">Robotics</span>|[On the Theoretical Limitations of Embedding-Based Retrieval](https://youtu.be/I7xagaD1pt0?si=mvDvHnDUvIREs7AG)|Mintaek Lim|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#fff7ed;color:#9a3412;border:1px solid #fdba74;">Multimodal AI</span>|[DeepSeek-OCR: Contexts Optical Compression](https://youtu.be/QUkNdfPA3C0)|Jaeik Kim|
|**Oct 17**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#fff7ed;color:#9a3412;border:1px solid #fdba74;">Multimodal AI</span>|[Video models are zero-shot learners and reasoners](https://youtu.be/2yDxmBOwfP4)|Jihwan Hong|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eef2ff;color:#3730a3;border:1px solid #c7d2fe;">Data Systems</span>|[Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://youtu.be/z6A0BLJS8UU)|Woojin Kim|
|**Oct 10**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Training and Inference on Any-Order Autoregressive Models the Right Way](https://youtu.be/HGUeMv7VVNE)|Yunseok Han|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#ecfeff;color:#155e75;border:1px solid #a5f3fc;">Agent Systems</span>|[Dynamic Planning for LLM-based Graphical User Interface Automation](https://youtu.be/QMjsZ73GBoc?si=dhaQVruEEVkzdGU8)|Hanjun Lee|
|**Sep 26**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#ecfdf5;color:#166534;border:1px solid #86efac;">Robotics</span>|[LLaDA-VLA: Vision-Language Diffusion Action Models](https://youtu.be/FVA3vbpteao)|Hoeun Lee|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#fef2f2;color:#991b1b;border:1px solid #fca5a5;">AI Safety</span>|[LLM Privacy Survey](https://www.youtube.com/watch?v=E0S3MVh0ghw)|Geun Choi|
|**Sep 19**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Mixture of Lookup Experts](https://youtu.be/vwOh7ET6oqw?si=B56gx3LvIIQbzC4z)|Sieun Hyeon|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Self-Questioning Language Models](https://youtu.be/WDfvbILy2-I?si=47c3FJ6ySmJVsUxv)|Yejoon Lee|
|**Sep 12**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#f0fdfa;color:#115e59;border:1px solid #99f6e4;">Representation Learning</span>|[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://youtu.be/5l9quUaJw7g)|Dogeun Kim|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#ecfdf5;color:#166534;border:1px solid #86efac;">Robotics</span>|[Latent Diffusion Planning for Imitation Learning](https://youtu.be/f0KKyqz1PH4?si=x5DmqINlO9fX6nKD)|Hyeonggeun Kim|
|**Aug 22**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#ecfeff;color:#155e75;border:1px solid #a5f3fc;">Agent Systems</span>|[Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment](https://youtu.be/Ce2VGxMOUNY)|Hanjun Lee|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#fff7ed;color:#9a3412;border:1px solid #fdba74;">Multimodal AI</span>|[Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts](https://youtu.be/J6PN_A5ggBk?si=k3C_KOJ4d0NdiMMK)|Woohyeon Park|
|**Aug 8**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity](https://youtu.be/lrJOB66FAgk?si=mqFs73OuB9Z4g0T9)|Jinhyeok Kim|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data](https://youtu.be/dvvfOHv4tOI?si=rqJ3ssYYPGVqKzMn)|Steve Cho|
|**Jul 25**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#f5f3ff;color:#5b21b6;border:1px solid #ddd6fe;">Reasoning</span>|[Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions](https://www.youtube.com/watch?v=4Yx4SESN88c)|Jaeik Kim|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://www.youtube.com/watch?v=KlA7JIAONO0)|Jusang Oh|
|**Jul 18**|<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#f0fdfa;color:#115e59;border:1px solid #99f6e4;">Representation Learning</span>|[On the Rankability of Visual Embeddings](https://www.youtube.com/watch?v=aKIzYYBRGj0)|Mintaek Lim|
||<span style="display:inline-block;padding:2px 8px;border-radius:9999px;font-size:12px;font-weight:600;line-height:1.2;background:#eff6ff;color:#1e40af;border:1px solid #bfdbfe;">LLMs</span>|[KV-efficient language models: MLA and sliding window attention](https://www.youtube.com/watch?v=t2VYbYKHsdQ)|Yejoon Lee|
